# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C1dlegHb5dBuMSbcfEsDM7uS-ujMVwU7
"""
import os
import sys
from pathlib import Path

# For√ßa a instala√ß√£o das depend√™ncias se estiver no Streamlit Cloud
if 'streamlit' in sys.modules:
    requirements = Path('requirements.txt')
    if requirements.exists():
        os.system(f'pip install --no-cache-dir -r {requirements}')
import streamlit as st
from googleapiclient.discovery import build
from wordcloud import WordCloud
import pandas as pd
import matplotlib.pyplot as plt
import re

def get_comments(youtube, query, max_videos=3):
    search_response = youtube.search().list(
        q=query, part='id', type='video', maxResults=max_videos
    ).execute()
    video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
    comments = []
    for video_id in video_ids:
        try:
            resp = youtube.commentThreads().list(
                videoId=video_id, part='snippet', maxResults=50, textFormat="plainText"
            ).execute()
            comments += [item['snippet']['topLevelComment']['snippet']['textDisplay']
                         for item in resp.get('items', [])]
        except:
            pass
    return comments

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|[^a-zA-Z√°√©√≠√≥√∫√£√µ√¢√™√Æ√¥√ª√ß\s]", "", text)
    return text

def generate_wordcloud(text):
    wc = WordCloud(width=800, height=400, background_color='white').generate(text)
    fig, ax = plt.subplots()
    ax.imshow(wc, interpolation='bilinear')
    ax.axis("off")
    st.pyplot(fig)

# Sua API Key
API_KEY = "AIzaSyAuXaVvnCS74quKwkEPNxIZFIah2HQei1E"

st.title("üîç Nuvem de Palavras - Coment√°rios do YouTube")
query = st.text_input("Digite um termo para busca (ex: m√∫sica brasileira):")

if st.button("Gerar Nuvem") and query:
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    comments = get_comments(youtube, query)
    if not comments:
        st.warning("Nenhum coment√°rio encontrado para esse termo.")
    else:
        clean_comments = [clean_text(c) for c in comments]
        all_text = " ".join(clean_comments)

        st.subheader("üåê Nuvem de Palavras")
        generate_wordcloud(all_text)

        st.subheader("üìã Palavras Mais Frequentes")
        freq = pd.Series(all_text.split()).value_counts().reset_index()
        freq.columns = ['Palavra', 'Frequ√™ncia']
        st.dataframe(freq.head(20))
